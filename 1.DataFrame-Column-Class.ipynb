{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a920e814-8151-4fb3-bde0-e41d11ea0d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#DataFrame Column Class\n",
    "\n",
    "** Data Source **\n",
    "* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n",
    "* Size on Disk: ~23 MB\n",
    "* Type: Compressed Parquet File\n",
    "* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Continue exploring the `DataFrame` set of APIs.\n",
    "* Introduce the `Column` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f47adb3c-f864-4204-b3ac-f5b94012d6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb166efe-b1dd-4dc7-8786-614b722a9c6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c01348-556e-4158-99b2-01af569a4f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n",
    "\n",
    "We will be using the same data source as our previous notebook.\n",
    "\n",
    "As such, we can go ahead and start by creating our initial `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05af237c-0a50-4d3b-bb56-7fb850c53c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#(source, sasEntity, sasToken) = getAzureDataSource()\n",
    "#spark.conf.set(sasEntity, sasToken)\n",
    "\n",
    "parquetFile = \"wasbs://spark-ui-simulator@dbacademy.blob.core.windows.net\" + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9129e8-62b7-4512-ba49-7bee0d5136b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[project: string, article: string, requests: int, bytes_served: bigint]\n"
     ]
    }
   ],
   "source": [
    "pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n",
    "  .read                     # Our DataFrameReader\n",
    "  .parquet(parquetFile)     # Returns an instance of DataFrame\n",
    "  .cache()                  # cache the data\n",
    ")\n",
    "print(pagecountsEnAllDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52987adb-fd6c-4437-a5ed-e85b42a8c8cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take another look at the number of records in our `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fd3e649-781d-40d5-85d7-66827ad4f419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record Count: 2,345,943\n"
     ]
    }
   ],
   "source": [
    "total = pagecountsEnAllDF.count()\n",
    "\n",
    "print(\"Record Count: {0:,}\".format( total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02b8ec9f-12a7-459c-8ba3-4a0ffb62c597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's take another peek at our data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfdf817-350c-496b-84bc-5a1ea52e7d33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pagecountsEnAllDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b637d7e-8a71-4f30-88d5-c505303025e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As we view the data, we can see that there is no real rhyme or reason as to how the data is sorted.\n",
    "* We cannot even tell if the column **project** is sorted - we are seeing only the first 1,000 of some 2.3 million records.\n",
    "* The column **article** is not sorted as evident by the article **A_Little_Boy_Lost** appearing between a bunch of articles starting with numbers and symbols.\n",
    "* The column **requests** is clearly not sorted.\n",
    "* And our **bytes_served** contains nothing but zeros.\n",
    "\n",
    "So let's start by sorting our data. In doing this, we can answer the following question:\n",
    "\n",
    "What are the top 10 most requested articles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "923160f9-adfe-4d00-9053-64f46afbcfd3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) orderBy(..) & sort(..)\n",
    "\n",
    "If you look at the API docs, `orderBy(..)` is described like this:\n",
    "> Returns a new Dataset sorted by the given expressions.\n",
    "\n",
    "Both `orderBy(..)` and `sort(..)` arrange all the records in the `DataFrame` as specified.\n",
    "* Like `distinct()` and `dropDuplicates()`, `sort(..)` and `orderBy(..)` are aliases for each other.\n",
    "  * `sort(..)` appealing to functional programmers.\n",
    "  * `orderBy(..)` appealing to developers with an SQL background.\n",
    "* Like `orderBy(..)` there are two variants of these two methods:\n",
    "  * `orderBy(Column)`\n",
    "  * `orderBy(String)`\n",
    "  * `sort(Column)`\n",
    "  * `sort(String)`\n",
    "\n",
    "All we need to do now is sort our previous `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc9ae8c-0300-47da-ac40-a86b83275539",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------+------------+\n|project|article         |requests|bytes_served|\n+-------+----------------+--------+------------+\n|en     |!Ora_language   |1       |0           |\n|en     |!Hukwe_language |1       |0           |\n|en     |!?Revolution!?  |1       |0           |\n|en     |!DOCTYPE        |1       |0           |\n|en     |!Kung_San       |1       |0           |\n|en     |!Ay,_caramba!   |1       |0           |\n|en     |!Tre!           |1       |0           |\n|en     |!T.O.O.H!       |1       |0           |\n|en     |!O!kung_language|1       |0           |\n|en     |!GÃ£!nge_language|1       |0           |\n+-------+----------------+--------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "sortedDF = (pagecountsEnAllDF\n",
    "  .orderBy(\"requests\")\n",
    ")\n",
    "sortedDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37944aa5-3613-46f3-826c-813fd3f605c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see, we are not sorting correctly.\n",
    "\n",
    "We need to reverse the sort.\n",
    "\n",
    "One might conclude that we could make a call like this:\n",
    "\n",
    "`pagecountsEnAllDF.orderBy(\"requests desc\")`\n",
    "\n",
    "Try it in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6872ae9d-1939-4c62-b6fc-2e126b4c5719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4170195121692590>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Uncomment and try this:\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mpagecountsEnAllDF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morderBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrequests desc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2681\u001B[0m, in \u001B[0;36mDataFrame.sort\u001B[0;34m(self, *cols, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   2590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msort\u001B[39m(\n",
       "\u001B[1;32m   2591\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: Union[\u001B[38;5;28mstr\u001B[39m, Column, List[Union[\u001B[38;5;28mstr\u001B[39m, Column]]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n",
       "\u001B[1;32m   2592\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   2593\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001B[39;00m\n",
       "\u001B[1;32m   2594\u001B[0m \n",
       "\u001B[1;32m   2595\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2679\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n",
       "\u001B[1;32m   2680\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 2681\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sort_cols\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2682\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `requests desc` cannot be resolved. Did you mean one of the following? [`requests`, `bytes_served`, `article`, `project`].;\n",
       "'Sort ['requests desc ASC NULLS FIRST], true\n",
       "+- Relation [project#2,article#3,requests#4,bytes_served#5L] parquet\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-4170195121692590>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Uncomment and try this:\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mpagecountsEnAllDF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morderBy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrequests desc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2681\u001B[0m, in \u001B[0;36mDataFrame.sort\u001B[0;34m(self, *cols, **kwargs)\u001B[0m\n\u001B[1;32m   2590\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msort\u001B[39m(\n\u001B[1;32m   2591\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39mcols: Union[\u001B[38;5;28mstr\u001B[39m, Column, List[Union[\u001B[38;5;28mstr\u001B[39m, Column]]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[1;32m   2592\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   2593\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` sorted by the specified column(s).\u001B[39;00m\n\u001B[1;32m   2594\u001B[0m \n\u001B[1;32m   2595\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2679\u001B[0m \u001B[38;5;124;03m    +---+-----+\u001B[39;00m\n\u001B[1;32m   2680\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2681\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sort_cols\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2682\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:194\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    190\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `requests desc` cannot be resolved. Did you mean one of the following? [`requests`, `bytes_served`, `article`, `project`].;\n'Sort ['requests desc ASC NULLS FIRST], true\n+- Relation [project#2,article#3,requests#4,bytes_served#5L] parquet\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `requests desc` cannot be resolved. Did you mean one of the following? [`requests`, `bytes_served`, `article`, `project`].;\n'Sort ['requests desc ASC NULLS FIRST], true\n+- Relation [project#2,article#3,requests#4,bytes_served#5L] parquet\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment and try this:\n",
    "pagecountsEnAllDF.orderBy(\"requests desc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b76ac9ae-f561-43df-a961-826f6301b3f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Why does this not work?\n",
    "* The `DataFrames` API is built upon an SQL engine.\n",
    "* There is a lot of familiarity with this API and SQL syntax in general.\n",
    "* The problem is that `orderBy(..)` expects the name of the column.\n",
    "* What we specified was an SQL expression in the form of **requests desc**.\n",
    "* What we need is a way to programmatically express such an expression.\n",
    "* This leads us to the second variant, `orderBy(Column)` and more specifically, the class `Column`.\n",
    "\n",
    "** *Note:* ** *Some of the calls in the `DataFrames` API actually accept SQL expressions.*<br/>\n",
    "*While these functions will appear in the docs as `someFunc(String)` it's very*<br>\n",
    "*important to thoroughly read and understand what the parameter actually represents.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe09283-ac08-42ab-beaf-54282edf62d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Column Class\n",
    "\n",
    "The `Column` class is an object that encompasses more than just the name of the column, but also column-level-transformations, such as sorting in a descending order.\n",
    "\n",
    "The first question to ask is how do I create a `Column` object?\n",
    "\n",
    "In Scala we have these options:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a3a7752-db44-4abc-8fd6-d54bffee1cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "** *Note:* ** *We are showing both the Scala and Python versions below for comparison.*<br/>\n",
    "*Make sure to run only the one cell for your notebook's default language (Scala or Python)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b59095-06fa-4177-89dd-a41b3c64ca2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "// Scala & Python both support accessing a column from a known DataFrame\n",
    "// Uncomment this if you are using the Scala version of this notebook\n",
    "// val columnA = pagecountsEnAllDF(\"requests\")    \n",
    "\n",
    "// This option is Scala specific, but is arugably the cleanest and easy to read.\n",
    "val columnB = $\"requests\"          \n",
    "\n",
    "// If we import ...sql.functions, we get a couple of more options:\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// This uses the col(..) function\n",
    "val columnC = col(\"requests\")\n",
    "\n",
    "// This uses the expr(..) function which parses an SQL Expression\n",
    "val columnD = expr(\"a + 1\")\n",
    "\n",
    "// This uses the lit(..) to create a literal (constant) value.\n",
    "val columnE = lit(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0e26f1c-643b-41e2-8857-affc37a7c709",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In Python we have these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2af883-83c1-4448-8626-b47fef94d2f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnC: Column<'requests'>\ncolumnD: Column<'(a + 1)'>\ncolumnE: Column<'abc'>\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "\n",
    "# Scala & Python both support accessing a column from a known DataFrame\n",
    "# Uncomment this if you are using the Python version of this notebook\n",
    "columnA = pagecountsEnAllDF[\"requests\"]\n",
    "\n",
    "# The $\"column-name\" version that works for Scala does not work in Python\n",
    "# columnB = $\"requests\"      \n",
    "\n",
    "# If we import ...sql.functions, we get a couple of more options:\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# This uses the col(..) function\n",
    "columnC = col(\"requests\")\n",
    "\n",
    "# This uses the expr(..) function which parses an SQL Expression\n",
    "columnD = expr(\"a + 1\")\n",
    "\n",
    "# This uses the lit(..) to create a literal (constant) value.\n",
    "columnE = lit(\"abc\")\n",
    "\n",
    "# Print the type of each attribute\n",
    "print(\"columnC: {}\".format(columnC))\n",
    "print(\"columnD: {}\".format(columnD))\n",
    "print(\"columnE: {}\".format(columnE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a0702be-47e3-4bac-aea1-3aa3cf49f1b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In the case of Scala, the cleanest version is the **$\"column-name\"** variant.\n",
    "\n",
    "In the case of Python, the cleanest version is the **col(\"column-name\")** variant.\n",
    "\n",
    "So with that, we can now create a `Column` object, and apply the `desc()` operation to it:\n",
    "\n",
    "** *Note:* ** *We are introducing `...sql.functions` specifically for creating `Column` objects.*<br/>\n",
    "*We will be reviewing the multitude of other commands available from this part of the API in future notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c54e064-9313-4b86-b687-596072011609",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column: Column<'requests DESC NULLS LAST'>\n"
     ]
    }
   ],
   "source": [
    "column = col(\"requests\").desc()\n",
    "\n",
    "# Print the column type\n",
    "print(\"column:\", column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3583c766-ce14-4051-835b-698e97f88fef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "And now we can piece it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a6fef7-c3b6-4951-a839-4eb5177e8d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+--------+------------+\n|project|article                    |requests|bytes_served|\n+-------+---------------------------+--------+------------+\n|en     |Main_Page                  |865692  |0           |\n|en.m   |Main_Page                  |176949  |0           |\n|en     |Special:Search             |76231   |0           |\n|en.m   |Donald_Trump               |59847   |0           |\n|en     |Midas                      |55210   |0           |\n|en     |Donald_Trump               |44640   |0           |\n|en.m   |-                          |44130   |0           |\n|en     |-                          |35663   |0           |\n|en.m   |Melania_Trump              |24183   |0           |\n|en     |Special:RecentChangesLinked|23419   |0           |\n+-------+---------------------------+--------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "sortedDescDF = (pagecountsEnAllDF\n",
    "  .orderBy( col(\"requests\").desc() )\n",
    ")  \n",
    "sortedDescDF.show(10, False) # The top 10 is good enough for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2faff71b-fde8-48e2-8a12-cac6c3d67285",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It should be of no surprise that the **Main_Page** (in both the Wikipedia and Wikimedia projects) is the most requested page.\n",
    "\n",
    "Followed shortly after that is **Special:Search**, Wikipedia's search page.\n",
    "\n",
    "And if you consider that this data was captured in the August before the 2016 presidential election, the Trumps will be one of the most requested pages on Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc8a54f-64e0-48e5-8094-0b68f648bb47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Review Column Class\n",
    "\n",
    "The `Column` objects provide us a programmatic way to build up SQL-ish expressions.\n",
    "\n",
    "Besides the `Column.desc()` operation we used above, we have a number of other operations that can be performed on a `Column` object.\n",
    "\n",
    "Here is a preview of the various functions - we will cover many of these as we progress through the class:\n",
    "\n",
    "**Column Functions**\n",
    "* Various mathematical functions such as add, subtract, multiply & divide\n",
    "* Various bitwise operators such as AND, OR & XOR\n",
    "* Various null tests such as `isNull()`, `isNotNull()` & `isNaN()`.\n",
    "* `as(..)`, `alias(..)` & `name(..)` - Returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).\n",
    "* `between(..)` - A boolean expression that is evaluated to true if the value of this expression is between the given columns.\n",
    "* `cast(..)` & `astype(..)` - Convert the column into type dataType.\n",
    "* `asc(..)` - Returns a sort expression based on the ascending order of the given column name.\n",
    "* `desc(..)` - Returns a sort expression based on the descending order of the given column name.\n",
    "* `startswith(..)` - String starts with.\n",
    "* `endswith(..)` - String ends with another string literal.\n",
    "* `isin(..)` - A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n",
    "* `like(..)` - SQL like expression\n",
    "* `rlike(..)` - SQL RLIKE expression (LIKE with Regex).\n",
    "* `substr(..)` - An expression that returns a substring.\n",
    "* `when(..)` & `otherwise(..)` - Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
    "\n",
    "The complete list of functions differs from language to language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fb91fa-6351-40df-92e7-17a8a148f2a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Start the next lesson, [Work with Column expressions]($./2.DataFrame-Column-Expressions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1.DataFrame-Column-Class",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

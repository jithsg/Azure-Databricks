{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a920e814-8151-4fb3-bde0-e41d11ea0d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#DataFrame Column Expressions\n",
    "\n",
    "** Data Source **\n",
    "* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n",
    "* Size on Disk: ~23 MB\n",
    "* Type: Compressed Parquet File\n",
    "* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Continue exploring the `DataFrame` set of APIs.\n",
    "* Continue to work with the `Column` class and introduce the `Row` class\n",
    "* Introduce the transformations...\n",
    "  * `orderBy(..)`\n",
    "  * `sort(..)`\n",
    "  * `filter(..)`\n",
    "  * `where(..)`\n",
    "* Introduce the actions...\n",
    "  * `collect()`\n",
    "  * `take(n)`\n",
    "  * `first()`\n",
    "  * `head()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f47adb3c-f864-4204-b3ac-f5b94012d6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb166efe-b1dd-4dc7-8786-614b722a9c6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5c01348-556e-4158-99b2-01af569a4f6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n",
    "\n",
    "We will be using the same data source as our previous notebook.\n",
    "\n",
    "As such, we can go ahead and start by creating our initial `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05af237c-0a50-4d3b-bb56-7fb850c53c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#(source, sasEntity, sasToken) = getAzureDataSource()\n",
    "#spark.conf.set(sasEntity, sasToken)\n",
    "\n",
    "parquetFile = \"wasbs://spark-ui-simulator@dbacademy.blob.core.windows.net\" + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9129e8-62b7-4512-ba49-7bee0d5136b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[project: string, article: string, requests: int, bytes_served: bigint]\n"
     ]
    }
   ],
   "source": [
    "pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n",
    "  .read                     # Our DataFrameReader\n",
    "  .parquet(parquetFile)     # Returns an instance of DataFrame\n",
    "  .cache()                  # cache the data\n",
    ")\n",
    "print(pagecountsEnAllDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc4af471-1e2c-448c-b18a-31ee84b5700c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's look at the data once more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e871500d-ca62-475e-a1fa-08836e3f1fd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------+--------+------------+\n|project|article                    |requests|bytes_served|\n+-------+---------------------------+--------+------------+\n|en     |Main_Page                  |865692  |0           |\n|en.m   |Main_Page                  |176949  |0           |\n|en     |Special:Search             |76231   |0           |\n|en.m   |Donald_Trump               |59847   |0           |\n|en     |Midas                      |55210   |0           |\n|en     |Donald_Trump               |44640   |0           |\n|en.m   |-                          |44130   |0           |\n|en     |-                          |35663   |0           |\n|en.m   |Melania_Trump              |24183   |0           |\n|en     |Special:RecentChangesLinked|23419   |0           |\n+-------+---------------------------+--------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "sortedDescDF = (pagecountsEnAllDF\n",
    "  .orderBy( col(\"requests\").desc() )\n",
    ")  \n",
    "sortedDescDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec04d79-02fd-4be3-bb6c-6d514b53d6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "In looking at the data, we can see multiple Wikipedia projects.\n",
    "\n",
    "What if we want to look at only the main Wikipedia project, **en**?\n",
    "\n",
    "For that, we will need to filter out some records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03f75508-efa2-4932-8453-447f87bc9cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) filter(..) & where(..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c4d3c50-27e7-43ff-a554-7e5ffcc40fb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "If you look at the API docs, `filter(..)` and `where(..)` are described like this:\n",
    "> Filters rows using the given condition.\n",
    "\n",
    "Both `filter(..)` and `where(..)` return a new dataset containing only those records for which the specified condition is true.\n",
    "* Like `distinct()` and `dropDuplicates()`, `filter(..)` and `where(..)` are aliases for each other.\n",
    "  * `filter(..)` appealing to functional programmers.\n",
    "  * `where(..)` appealing to developers with an SQL background.\n",
    "* Like `orderBy(..)` there are two variants of these two methods:\n",
    "  * `filter(Column)`\n",
    "  * `filter(String)`\n",
    "  * `where(Column)`\n",
    "  * `where(String)`\n",
    "* Unlike `orderBy(String)` which requires a column name, `filter(String)` and `where(String)` both expect an SQL expression.\n",
    "\n",
    "Let's start by looking at the variant using an SQL expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd4f751-1c87-414e-ba5d-93cf5503f257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### filter(..) & where(..) w/SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaf8a30-0df7-4dd4-b10d-03cbd23e9e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+--------+------------+\n|project|article                                  |requests|bytes_served|\n+-------+-----------------------------------------+--------+------------+\n|en     |Main_Page                                |865692  |0           |\n|en     |Special:Search                           |76231   |0           |\n|en     |Midas                                    |55210   |0           |\n|en     |Donald_Trump                             |44640   |0           |\n|en     |-                                        |35663   |0           |\n|en     |Special:RecentChangesLinked              |23419   |0           |\n|en     |404.php                                  |20363   |0           |\n|en     |United_States_presidential_election,_2016|19401   |0           |\n|en     |Melania_Trump                            |15220   |0           |\n|en     |Special:CreateAccount                    |11958   |0           |\n+-------+-----------------------------------------+--------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "whereDF = (sortedDescDF\n",
    "  .where( \"project = 'en'\" )\n",
    ")\n",
    "whereDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c963c3-c180-45d0-a806-ccfe736a7e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we are only looking at the main Wikipedia articles, we get a better picture of the most popular articles on Wikipedia.\n",
    "\n",
    "Next, let's take a look at the second variant that takes a `Column` object as its first parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b74c5bf-8112-4a41-afe7-c9d1be51512a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### filter(..) & where(..) w/Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d34c266-6d5f-4c5a-92dc-81a064ef30d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+--------+------------+\n|project|article                                  |requests|bytes_served|\n+-------+-----------------------------------------+--------+------------+\n|en     |Main_Page                                |865692  |0           |\n|en     |Special:Search                           |76231   |0           |\n|en     |Midas                                    |55210   |0           |\n|en     |Donald_Trump                             |44640   |0           |\n|en     |-                                        |35663   |0           |\n|en     |Special:RecentChangesLinked              |23419   |0           |\n|en     |404.php                                  |20363   |0           |\n|en     |United_States_presidential_election,_2016|19401   |0           |\n|en     |Melania_Trump                            |15220   |0           |\n|en     |Special:CreateAccount                    |11958   |0           |\n+-------+-----------------------------------------+--------+------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "filteredDF = (sortedDescDF\n",
    "  .filter( col(\"project\") == \"en\")\n",
    ")\n",
    "filteredDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b8243a1-c257-4376-9fe4-16c39de40406",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A Scala Issue..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f472673-4995-4552-8342-cf2f05b23869",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "With Python, this is pretty straight forward.\n",
    "\n",
    "But in Scala... notice anything unusual in that last command?\n",
    "\n",
    "**Question:** In most every programming language, what is a single equals sign (=) used for?\n",
    "\n",
    "**Question:** What are two equal signs (==) used for?\n",
    "\n",
    "**Question:** \n",
    "* Considering that transformations are lazy...\n",
    "* And the == operator executes now...\n",
    "* And `filter(..)` and `where(..)` require us to pass a `Column` object...\n",
    "* What would be wrong with `$\"project\" == \"en\"`?\n",
    "\n",
    "Try it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0689b419-f426-48c3-b6ef-9b146af31b69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "$\"project\" == \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d930f28a-e6e9-4f4f-baf8-e84d63685f93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Compare that to the following call..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e91a71e6-4bd9-4a7b-baf2-62ef0aca4638",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "$\"project\" === \"en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc09b38-5b58-4716-bbb8-c861d6ec2e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at the Scala Doc for the `Column` object. </br>\n",
    "\n",
    "| \"Operator\" | Function |\n",
    "|:----------:| -------- |\n",
    "| === | Equality test |\n",
    "| !== | Deprecated inequality test |\n",
    "| =!= | Inequality test |\n",
    "| <=> | Null safe equality test |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c700d027-a1a9-43b6-af48-97cdd2f88464",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The Solution..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d6c1ae-0264-41d4-b6b0-fca4544b6f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "With that behind us, we can clearly **see** the top ten most requested articles.\n",
    "\n",
    "But what if we need to **programmatically** extract the value of the most requested article's name and its number of requests?\n",
    "\n",
    "That is to say, how do we get the first record, and from there...\n",
    "* the value of the second column, **article**, as a string...\n",
    "* the value of the third column, **requests**, as an integer...\n",
    "\n",
    "Before we proceed, let's apply another filter to get rid of **Main_Page** and anything starting with **Special:** - they're just noise to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06a9453-15a3-41e2-bdce-97a6180407a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------+--------+\n|project|article                                  |requests|\n+-------+-----------------------------------------+--------+\n|en     |Midas                                    |55210   |\n|en     |Donald_Trump                             |44640   |\n|en     |404.php                                  |20363   |\n|en     |United_States_presidential_election,_2016|19401   |\n|en     |Melania_Trump                            |15220   |\n|en     |Electoral_College_(United_States)        |11788   |\n|en     |United_States_presidential_election,_2012|11267   |\n|en     |AMGTV                                    |10775   |\n|en     |Proyecto_40                              |9723    |\n|en     |United_States_presidential_election,_2008|7481    |\n+-------+-----------------------------------------+--------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "articlesDF = (filteredDF\n",
    "  .drop(\"bytes_served\")\n",
    "  .filter( col(\"article\") != \"Main_Page\")\n",
    "  .filter( col(\"article\") != \"-\")\n",
    "  .filter( col(\"article\").startswith(\"Special:\") == False)\n",
    ")\n",
    "articlesDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bf0556-6f81-4b96-b330-8bacbb2382b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) first() & head()\n",
    "\n",
    "If you look at the API docs, both `first(..)` and `head(..)` are described like this:\n",
    "> Returns the first row.\n",
    "\n",
    "Just like `distinct()` & `dropDuplicates()` are aliases for each other, so are `first(..)` and `head(..)`.\n",
    "\n",
    "However, unlike `distinct()` & `dropDuplicates()` which are **transformations** `first(..)` and `head(..)` are **actions**.\n",
    "\n",
    "Once all processing is done, these methods return the object backing the first record.\n",
    "\n",
    "In the case of `DataFrames` (both Scala and Python) that object is a `Row`.\n",
    "\n",
    "In the case of `Datasets` (the strongly typed version of `DataFrames` in Scala and Java), the object may be a `Row`, a `String`, a `Customer`, a `PendingApplication` or any number of custom objects.\n",
    "\n",
    "Focusing strictly on the `DataFrame` API for now, let's take a look at a call with `head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e8e7bd-02e5-4753-bdaf-ab476190d23e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(project='en', article='Midas', requests=55210)\n"
     ]
    }
   ],
   "source": [
    "firstRow = articlesDF.first()\n",
    "\n",
    "print(firstRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34152700-e660-4d9e-bf0a-e75b878328e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) The Row Class\n",
    "\n",
    "Now that we have a reference to the object backing the first row (or any row), we can use it to extract the data for each column.\n",
    "\n",
    "Before we do, let's take a look at the API docs for the `Row` class.\n",
    "\n",
    "At the heart of it, we are simply going to ask for the value of the object in column N via `Row.get(i)`.\n",
    "\n",
    "Python being a loosely typed language, the return value is of no real consequence.\n",
    "\n",
    "However, Scala is going to return an object of type `Any`. In Java, this would be an object of type `Object`.\n",
    "\n",
    "What we need (at least for Scala), especially if the data type matters in cases of performing mathematical operations on the value, we need to call one of the other methods:\n",
    "* `getAs[T](i):T`\n",
    "* `getDate(i):Date`\n",
    "* `getString(i):String`\n",
    "* `getInt(i):Int`\n",
    "* `getLong(i):Long`\n",
    "\n",
    "We can now put it all together to get the number of requests for the most requested project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a633150a-027a-482a-971c-cbe72e8691ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Requested Article: \"Midas\" with 55,210 requests\n"
     ]
    }
   ],
   "source": [
    "article = firstRow['article']\n",
    "total = firstRow['requests']\n",
    "\n",
    "print(\"Most Requested Article: \\\"{0}\\\" with {1:,} requests\".format( article, total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dcac4fc-4a9a-4c72-b7c8-0e9f9aff9d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) collect()\n",
    "\n",
    "If you look at the API docs, `collect(..)` is described like this:\n",
    "> Returns an array that contains all of Rows in this Dataset.\n",
    "\n",
    "`collect()` returns a collection of the specific type backing each record of the `DataFrame`.\n",
    "* In the case of Python, this is always the `Row` object.\n",
    "* In the case of Scala, this is also a `Row` object.\n",
    "* If the `DataFrame` was converted to a `Dataset` the backing object would be the user-specified object.\n",
    "\n",
    "Building on our last example, let's take the top 10 records and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa674023-017a-439c-ba0f-8b725cfb4c3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n<body>\n  <h1>Top 10 Articles</h1>\n  <ol>\n    <li><b>Midas</b> 55,210 requests</li>\n    <li><b>Donald_Trump</b> 44,640 requests</li>\n    <li><b>404.php</b> 20,363 requests</li>\n    <li><b>United_States_presidential_election,_2016</b> 19,401 requests</li>\n    <li><b>Melania_Trump</b> 15,220 requests</li>\n    <li><b>Electoral_College_(United_States)</b> 11,788 requests</li>\n    <li><b>United_States_presidential_election,_2012</b> 11,267 requests</li>\n    <li><b>AMGTV</b> 10,775 requests</li>\n    <li><b>Proyecto_40</b> 9,723 requests</li>\n    <li><b>United_States_presidential_election,_2008</b> 7,481 requests</li>\n  </ol>\n</body>\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<body>\n",
       "  <h1>Top 10 Articles</h1>\n",
       "  <ol>\n",
       "    <li><b>Midas</b> 55,210 requests</li>\n",
       "    <li><b>Donald_Trump</b> 44,640 requests</li>\n",
       "    <li><b>404.php</b> 20,363 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2016</b> 19,401 requests</li>\n",
       "    <li><b>Melania_Trump</b> 15,220 requests</li>\n",
       "    <li><b>Electoral_College_(United_States)</b> 11,788 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2012</b> 11,267 requests</li>\n",
       "    <li><b>AMGTV</b> 10,775 requests</li>\n",
       "    <li><b>Proyecto_40</b> 9,723 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2008</b> 7,481 requests</li>\n",
       "  </ol>\n",
       "</body>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = (articlesDF\n",
    "  .limit(10)           # We only want the first 10 records.\n",
    "  .collect()           # The action returning all records in the DataFrame\n",
    ")\n",
    "\n",
    "# rows is an Array. Now in the driver, \n",
    "# we can just loop over the array and print 'em out.\n",
    "\n",
    "listItems = \"\"\n",
    "for row in rows:\n",
    "  project = row['article']\n",
    "  total = row['requests']\n",
    "  listItems += \"    <li><b>{}</b> {:0,d} requests</li>\\n\".format(project, total)\n",
    "  \n",
    "html = \"\"\"\n",
    "<body>\n",
    "  <h1>Top 10 Articles</h1>\n",
    "  <ol>\n",
    "    %s\n",
    "  </ol>\n",
    "</body>\n",
    "\"\"\" % (listItems.strip())\n",
    "\n",
    "print(html)\n",
    "\n",
    "# UNCOMMENT FOR A PRETTIER PRESENTATION\n",
    "displayHTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab199d03-57d9-497d-903a-58ce975f1a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) take(n)\n",
    "\n",
    "If you look at the API docs, `take(n)` is described like this:\n",
    "> Returns the first n rows in the Dataset.\n",
    "\n",
    "`take(n)` returns a collection of the first N records of the specific type backing each record of the `DataFrame`.\n",
    "* In the case of Python, this is always the `Row` object.\n",
    "* In the case of Scala, this is also a `Row` object.\n",
    "* If the `DataFrame` was converted to a `Dataset` the backing object would be the user-specified object.\n",
    "\n",
    "In short, it's the same basic function as `collect()` except you specify as the first parameter the number of records to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d02a47c9-d36c-4ab7-8be6-096c63c769d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n<body>\n  <h1>Top 10 Articles</h1>\n  <ol>\n    <li><b>Midas</b> 55,210 requests</li>\n    <li><b>Donald_Trump</b> 44,640 requests</li>\n    <li><b>404.php</b> 20,363 requests</li>\n    <li><b>United_States_presidential_election,_2016</b> 19,401 requests</li>\n    <li><b>Melania_Trump</b> 15,220 requests</li>\n    <li><b>Electoral_College_(United_States)</b> 11,788 requests</li>\n    <li><b>United_States_presidential_election,_2012</b> 11,267 requests</li>\n    <li><b>AMGTV</b> 10,775 requests</li>\n    <li><b>Proyecto_40</b> 9,723 requests</li>\n    <li><b>United_States_presidential_election,_2008</b> 7,481 requests</li>\n  </ol>\n</body>\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<body>\n",
       "  <h1>Top 10 Articles</h1>\n",
       "  <ol>\n",
       "    <li><b>Midas</b> 55,210 requests</li>\n",
       "    <li><b>Donald_Trump</b> 44,640 requests</li>\n",
       "    <li><b>404.php</b> 20,363 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2016</b> 19,401 requests</li>\n",
       "    <li><b>Melania_Trump</b> 15,220 requests</li>\n",
       "    <li><b>Electoral_College_(United_States)</b> 11,788 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2012</b> 11,267 requests</li>\n",
       "    <li><b>AMGTV</b> 10,775 requests</li>\n",
       "    <li><b>Proyecto_40</b> 9,723 requests</li>\n",
       "    <li><b>United_States_presidential_election,_2008</b> 7,481 requests</li>\n",
       "  </ol>\n",
       "</body>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rows = articlesDF.take(10)\n",
    "\n",
    "# rows is an Array. Now in the driver, \n",
    "# we can just loop over the array and print 'em out.\n",
    "\n",
    "listItems = \"\"\n",
    "for row in rows:\n",
    "  project = row['article']\n",
    "  total = row['requests']\n",
    "  listItems += \"    <li><b>{}</b> {:0,d} requests</li>\\n\".format(project, total)\n",
    "  \n",
    "html = \"\"\"\n",
    "<body>\n",
    "  <h1>Top 10 Articles</h1>\n",
    "  <ol>\n",
    "    %s\n",
    "  </ol>\n",
    "</body>\n",
    "\"\"\" % (listItems.strip())\n",
    "\n",
    "print(html)\n",
    "\n",
    "# UNCOMMENT FOR A PRETTIER PRESENTATION\n",
    "displayHTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99a06dc0-864b-4e8d-924a-8be6d27ee1d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) DataFrame vs Dataset\n",
    "\n",
    "We've been alluding to `Datasets` off and on. \n",
    "\n",
    "The following example demonstrates how to convert a `DataFrame` to a `Dataset`.\n",
    "\n",
    "And when compared to the previous example, helps to illustrate the difference/relationship between the two.\n",
    "\n",
    "** *Note:* ** *As a reminder, `Datasets` are a Java and Scala concept and brings to those languages the type safety that *<br/>\n",
    "*is lost with `DataFrame`, or rather, `Dataset[Row]`. Python and R have no such concept because they are loosely typed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bc54096-0e22-4166-a33e-ce7465dd346a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before we demonstrate this, let's review all our transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ac17a9-4e45-4948-822a-26da2f0d0874",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val (source, sasEntity, sasToken) = getAzureDataSource()\n",
    "spark.conf.set(sasEntity, sasToken)\n",
    "\n",
    "val parquetFile = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe24a169-d98a-463c-a4c9-7ae4f0be4315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "val articlesDF = spark                          // Our SparkSession & Entry Point\n",
    "  .read                                         // Our DataFrameReader\n",
    "  .parquet(parquetFile)                         // Creates a DataFrame from a parquet file\n",
    "  .filter( $\"project\" === \"en\")                 // Include only the \"en\" project\n",
    "  .filter($\"article\" =!= \"Main_Page\")           // Exclude the Wikipedia Main Page\n",
    "  .filter($\"article\" =!= \"-\")                   // Exclude some \"weird\" article\n",
    "  .filter( ! $\"article\".startsWith(\"Special:\")) // Exclude all the \"special\" articles\n",
    "  .drop(\"bytes_served\")                         // We just don't need this column\n",
    "  .orderBy( $\"requests\".desc )                  // Sort by requests descending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81cd1925-2f55-4c54-a7c5-780e31c45bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice above that `articlesDF` is a `Dataset` of type `Row`.\n",
    "\n",
    "Next, create the case class `WikiReq`. \n",
    "\n",
    "A little later we can convert this `DataFrame` to a `Dataset` of type `WikiReq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "588c812b-202c-406e-b028-d03e8b71fbc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// the name and data type of the case class must match the schema they will be converted from.\n",
    "case class WikiReq (project:String, article:String, requests:Int)\n",
    "\n",
    "articlesDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a54c1cd-3dad-4e41-91e5-41fa580cb49c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instead of the `Row` object, we can now back each record with our new `WikiReq` class.\n",
    "\n",
    "And we can see the conversion from `DataFrames` to `Datasets` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfe9e8a-f8d7-4270-8876-37863048a831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "val articlesDS = articlesDF.as[WikiReq]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7633cd-8e5d-4157-a7b8-430f62514f3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Make note of the data type: **org.apache.spark.sql.Dataset[WikiReq]**\n",
    "\n",
    "Compare that to a `DataFrame`: **org.apache.spark.sql.Dataset[Row]**\n",
    "\n",
    "Now when we ask for the first 10, we won't get an array of `Row` objects but instead an array of `WikiReq` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72324eef-995a-4b9e-89a8-4d431374fc1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val wikiReqs = articlesDS.take(10)\n",
    "\n",
    "// wikiReqs is an Array of WikiReqs. Now in the driver, \n",
    "// we can just loop over the array and print 'em out.\n",
    "\n",
    "var listItems = \"\"\n",
    "for (wikiReq <- wikiReqs) {\n",
    "  // Notice how we don't relaly need temp variables?\n",
    "  // Or more specifically, we don't need to cast.\n",
    "  listItems += \"    <li><b>%s</b> %,d requests</li>%n\".format(wikiReq.article, wikiReq.requests)\n",
    "}\n",
    "\n",
    "var html = s\"\"\"\n",
    "<body>\n",
    "  <h1>Top 10 Articles</h1>\n",
    "  <ol>\n",
    "    ${listItems.trim()}\n",
    "  </ol>\n",
    "</body>\n",
    "\"\"\"\n",
    "\n",
    "println(html)\n",
    "println(\"-\"*80)\n",
    "\n",
    "// UNCOMMENT FOR A PRETTIER PRESENTATION\n",
    "// displayHTML(html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.DataFrame-Column-Expressions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
